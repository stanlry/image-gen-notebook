{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies\n",
    "Install required packages including diffusers, transformers, torch, and necessary dependencies. Include environment-specific installations for Colab vs local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "\n",
    "# Check if the notebook is running in Google Colab\n",
    "!pip3 install diffusers transformers\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment Check\n",
    "Check if running in Colab or local environment, set device (CPU/GPU/MPS), and configure appropriate paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment Check\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if the notebook is running in Google Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Set device to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Running on Google Colab\")\n",
    "else:\n",
    "    # Set device to GPU if available, otherwise CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(\"Running on local environment\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_dir = \"models\"\n",
    "checkpoint_dir = os.path.join(model_dir, \"checkpoint\")\n",
    "lora_dir = os.path.join(model_dir, \"lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Load SDXL Model\n",
    "Download and load the base SDXL model using diffusers library, with proper error handling and memory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Models\n",
    "\n",
    "default_models = {\n",
    "    \"dynavision-xl\": \"https://civitai.com/api/download/models/297740?type=Model&format=SafeTensor&size=pruned&fp=fp16\",\n",
    "    \"realvis-xl\": \"https://civitai.com/api/download/models/798204?type=Model&format=SafeTensor&size=full&fp=fp16\"\n",
    "}\n",
    "\n",
    "for model_name, model_url in default_models.items():\n",
    "    print(f\"Downloading {model_name} model\")\n",
    "    !aria2c --auto-file-renaming=false --continue --dir=models/checkpoint/ $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from diffusers import StableDiffusionPipeline, AutoPipelineForText2Image\n",
    "\n",
    "# Define the model ID for the SDXL model\n",
    "# @markdown ### **Model Config**\n",
    "model_id = \"stable-diffusion-v1.5\" #@param {type:\"string\"}\n",
    "\n",
    "# List all models in the model directory\n",
    "models = os.listdir(checkpoint_dir)\n",
    "for model in models:\n",
    "    print(model)\n",
    "\n",
    "# try:\n",
    "#     # Load the SDXL model\n",
    "#     AutoPipelineForText2Image.from_pretrained()\n",
    "#     pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "#     pipe = pipe.to(device)\n",
    "#     print(\"Model loaded successfully\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LoRA Models\n",
    "Function to load and apply LoRA models to the base SDXL model, with support for multiple LoRA weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA Models\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_lora_models(base_model, lora_weights_list):\n",
    "    \"\"\"\n",
    "    Load and apply LoRA models to the base SDXL model.\n",
    "    \n",
    "    Args:\n",
    "    - base_model: The base SDXL model.\n",
    "    - lora_weights_list: List of paths or identifiers for LoRA weights.\n",
    "    \n",
    "    Returns:\n",
    "    - The base model with LoRA weights applied.\n",
    "    \"\"\"\n",
    "    for lora_weights in lora_weights_list:\n",
    "        try:\n",
    "            # Load the LoRA model\n",
    "            lora_model = AutoModelForCausalLM.from_pretrained(lora_weights)\n",
    "            # Apply LoRA weights to the base model\n",
    "            base_model = lora_model.apply_lora(base_model)\n",
    "            print(f\"LoRA weights from {lora_weights} applied successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading LoRA weights from {lora_weights}: {e}\")\n",
    "    return base_model\n",
    "\n",
    "# Example usage\n",
    "lora_weights_list = [\"path/to/lora/weights1\", \"path/to/lora/weights2\"]\n",
    "pipe = load_lora_models(pipe, lora_weights_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Image Generation Function\n",
    "Create a function that handles prompt processing, inference pipeline setup, and image generation with customizable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Image Generation Function\n",
    "\n",
    "def generate_image(prompt, num_inference_steps=50, guidance_scale=7.5, height=512, width=512):\n",
    "    \"\"\"\n",
    "    Generate an image from a text prompt using the SDXL model with LoRA models.\n",
    "    \n",
    "    Args:\n",
    "    - prompt: The text prompt for image generation.\n",
    "    - num_inference_steps: Number of inference steps for the generation process.\n",
    "    - guidance_scale: Guidance scale for classifier-free guidance.\n",
    "    - height: Height of the generated image.\n",
    "    - width: Width of the generated image.\n",
    "    \n",
    "    Returns:\n",
    "    - Generated image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate image\n",
    "        image = pipe(prompt, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, height=height, width=width).images[0]\n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "prompt = \"A futuristic cityscape at sunset\"\n",
    "generated_image = generate_image(prompt)\n",
    "if generated_image:\n",
    "    generated_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Images with Different Prompts\n",
    "Demonstrate image generation with various prompts and LoRA combinations, including parameter adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Images with Different Prompts\n",
    "\n",
    "# Define a list of prompts\n",
    "prompts = [\n",
    "    \"A futuristic cityscape at sunset\",\n",
    "    \"A serene mountain landscape with a river\",\n",
    "    \"A bustling market in a medieval town\",\n",
    "    \"A spaceship flying through a nebula\",\n",
    "    \"A fantasy castle surrounded by dragons\"\n",
    "]\n",
    "\n",
    "# Define a list of LoRA weight combinations (for demonstration purposes, using the same weights)\n",
    "lora_combinations = [\n",
    "    [\"path/to/lora/weights1\"],\n",
    "    [\"path/to/lora/weights2\"],\n",
    "    [\"path/to/lora/weights1\", \"path/to/lora/weights2\"]\n",
    "]\n",
    "\n",
    "# Generate and display images for each prompt and LoRA combination\n",
    "for prompt in prompts:\n",
    "    for lora_weights in lora_combinations:\n",
    "        # Load the base model with the current LoRA weights\n",
    "        pipe_with_lora = load_lora_models(pipe, lora_weights)\n",
    "        \n",
    "        # Generate the image\n",
    "        generated_image = generate_image(prompt)\n",
    "        \n",
    "        # Display the image\n",
    "        if generated_image:\n",
    "            print(f\"Prompt: {prompt} | LoRA Weights: {lora_weights}\")\n",
    "            generated_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Generated Images\n",
    "Save generated images with metadata, including the prompt and parameters used for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Generated Images\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "def save_image_with_metadata(image, prompt, parameters, save_dir=\"generated_images\"):\n",
    "    \"\"\"\n",
    "    Save the generated image along with its metadata.\n",
    "    \n",
    "    Args:\n",
    "    - image: The generated image to save.\n",
    "    - prompt: The text prompt used for image generation.\n",
    "    - parameters: Dictionary of parameters used for generation.\n",
    "    - save_dir: Directory to save the images and metadata.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Define the image filename\n",
    "    image_filename = os.path.join(save_dir, f\"{prompt[:50].replace(' ', '_')}.png\")\n",
    "    \n",
    "    # Save the image\n",
    "    image.save(image_filename)\n",
    "    \n",
    "    # Define the metadata filename\n",
    "    metadata_filename = os.path.join(save_dir, f\"{prompt[:50].replace(' ', '_')}.json\")\n",
    "    \n",
    "    # Save the metadata\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump({\n",
    "            \"prompt\": prompt,\n",
    "            \"parameters\": parameters\n",
    "        }, f, indent=4)\n",
    "    \n",
    "    print(f\"Image and metadata saved: {image_filename}, {metadata_filename}\")\n",
    "\n",
    "# Example usage\n",
    "if generated_image:\n",
    "    parameters = {\n",
    "        \"num_inference_steps\": 50,\n",
    "        \"guidance_scale\": 7.5,\n",
    "        \"height\": 512,\n",
    "        \"width\": 512\n",
    "    }\n",
    "    save_image_with_metadata(generated_image, prompt, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
